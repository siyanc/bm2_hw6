---
title: "DS2_HW3"
author: "Siyan Chen"
date: "4/6/2019"
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ISLR)# data
library(caret) # featureplot
library(glmnet) # glm
library(e1071)# confusion matrix
library(pROC)
library(mlbench)
library(AppliedPredictiveModeling)# transparentTheme
library(randomForest) # KNN
df = Weekly
head(df)
```

### (a)  Produce some graphical summaries of theWeeklydata.

```{r}
transparentTheme(trans = .4)
featurePlot(x = df[, 2:7],
            y = df$Direction,
            scales = list(x=list(relation = "free"),
                          y=list(relation = "free")),
            plot = "density", pch = "|")
```

### (b) Use the full data set to perform a logistic regression withDirectionas the responseand the five Lagv ariables plus Volumeas predictors.  Do any of the predictors appearto be statistically significant?  If so, which ones?

```{r}
set.seed(1)
rowTrain = createDataPartition(y = df$Direction,
                               p = 0.75,
                               list = FALSE)
glm_fit = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
              data = df,
              subset = rowTrain,
              family = binomial)
contrasts(df$Direction)
summary(glm_fit)
```
Yes, Lag1, Lag2 and Intercept 

### (c)  Compute  the  confusion  matrix  and  overall  fraction  of  correct  predictions.   Briefly explain what the confusion matrix is telling you.

```{r}
# Bayes classigier(cutoff 0.5)
test.pred.prob = predict(glm_fit, newdata = df[-rowTrain,],
                         type = "response")
test.pred = rep("Down", length(test.pred.prob))
test.pred[test.pred.prob>0.5] = "Up"

# confustionMatrix
confusionMatrix(data = as.factor(test.pred),
                reference = as.factor(df$Direction[-rowTrain]),
                positive = "Down")

```

### (d)  Plot the ROC curve using the predicted probability from logistic regression and reportthe AUC.

```{r}
roc_glm = roc(df$Direction[-rowTrain], test.pred.prob)
plot(roc_glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc_glm), col = 4, add = TRUE)
```



### (e)  Now fit the logistic regression model using a training data period from 1990 to 2008,with Lag1 and Lag2 as the predictors.  Plot the ROC curve using the held out data(that is, the data from 2009 and 2010) and report the AUC.

```{r}
train_subset = df %>% 
  filter(1990<=Year& Year<=2008)
test_subset = anti_join(df, train_subset)
rowtrain = train_subset$Direction
rowtest = test_subset$Direction

glm_fit1 = glm(Direction~ Lag1 + Lag2,
               data = train_subset,
               family = binomial)
summary(glm_fit1)
contrasts(train_subset$Direction)


pred.test.value = predict(glm_fit1,
                          newdata = test_subset,
                          type = "response")
# Bayes Method Cutoff
pred.test = rep("Down", length(pred.test.value))
pred.test[pred.test.value>0.5] = "Up"

# Confusion Matrix
confusionMatrix(data = as.factor(pred.test), 
                reference = as.factor(rowtest),
                positive = "Up")
# ROC
roc1 = roc(test_subset$Direction, pred.test.value)
plot(roc1, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc1), col = 4, add =TRUE)

```



### (f)  Repeat (e) using LDA and QDA.

```{r}
#Discriminant analysis
## LDA
library(MASS)
lda.fit = lda(Direction ~ Lag1 + Lag2,
              data = train_subset)
plot(lda.fit)

# evaluate the test set performance using roc
lda.pred = predict(lda.fit, newdata = test_subset)
head(lda.pred$posterior)
roc.lda = roc(test_subset$Direction,lda.pred$posterior[,2],
              levels = c("Down","Up"))
plot(roc.lda, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.lda), col = 4, add =TRUE)

## QDA
qda.fit = qda(Direction ~ Lag1 + Lag2,
              data = train_subset)
qda.pred = predict(qda.fit, newdata = test_subset)
head(qda.pred$posterior)
roc.qda = roc(test_subset$Direction, qda.pred$posterior[,2],
              levels = c("Down","Up"))
plot(roc.qda, legacy.axix = TRUE, print.auc = TRUE)
plot(smooth(roc.qda), col = 4, add = TRUE)


 
```


### (g)  Repeat (e) using KNN. Briefly discuss your results.

```{r}
# neighbor for classfication 
# Using caret
ctrl <-trainControl(method = "repeatedcv",repeats = 5,summaryFunction = twoClassSummary,classProbs = TRUE)

model.knn = train(x = train_subset[, 2:3],
                  y = train_subset$Direction,
                  method = "knn",
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(k = seq(1,100, by = 5)),
                  trControl = ctrl)
ggplot(model.knn)
# predict.train DO CENTRAL AND SCALE AUTOMATICALLY
# # how to central and scale here ?
# predic_knn = predict.train(model.knn, testX = test_subset[, 2:3],
#             testY = test_subset$Direction)


predic_knn1 = predict(model.knn$finalModel, newdata = test_subset[, 2:3])

roc_knn = roc(test_subset$Direction, predic_knn1[,2],
              levels = c("Down","Up"))
plot(roc_knn, legacy.axis = TRUE, print.auc = TRUE)
plot(smooth(roc_knn), col = 4, add = TRUE)


```

